{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import snntorch as snn\n",
    "import matplotlib.pyplot as plt\n",
    "from snntorch import surrogate\n",
    "from snntorch import spikegen\n",
    "from snntorch import functional\n",
    "from snntorch import LIF\n",
    "from snntorch import spikeplot as splt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (8156, 250, 700)\n",
      "dataset shape:  (2264, 250, 700)\n"
     ]
    }
   ],
   "source": [
    "# Dataset generated from \n",
    "torch.manual_seed(2)\n",
    "\n",
    "train_X = np.load('data/trainX_4ms.npy')\n",
    "train_y = np.load('data/trainY_4ms.npy').astype(np.float64)\n",
    "\n",
    "test_X = np.load('data/testX_4ms.npy')\n",
    "test_y = np.load('data/testY_4ms.npy').astype(np.float64)\n",
    "\n",
    "print('dataset shape: ', train_X.shape) #(shape stands for number of samples, time steps, number of neurons / features)\n",
    "print('dataset shape: ', test_X.shape) #(shape stands for number of samples, time steps, number of neurons / features)\n",
    "\n",
    "tensor_trainX = torch.Tensor(train_X)  # transform to torch tensor\n",
    "tensor_trainY = torch.Tensor(train_y)\n",
    "train_dataset = data.TensorDataset(tensor_trainX, tensor_trainY)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size = 64, shuffle=True)\n",
    "tensor_testX = torch.Tensor(test_X)  # transform to torch tensor\n",
    "tensor_testY = torch.Tensor(test_y)\n",
    "test_dataset = data.TensorDataset(tensor_testX, tensor_testY)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spike_tensor(spk_tensor, title):\n",
    "    # Generate the plot\n",
    "    spk_tensor = spk_tensor.T\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Plot spikes\n",
    "    splt.raster(spk_tensor, ax, s=0.5, c=\"black\")  # Transpose to align with neurons on y-axis\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    ax.set_ylabel(\"Neuron\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def simple_branching_param(bin_size, spikes):  # spikes in shape of [units, time]\n",
    "    run_time = 64\n",
    "    nbins = 64\n",
    "    # nbins = int(np.round(run_time / bin_size))\n",
    "\n",
    "    # for every pair of timesteps, determine the number of ancestors\n",
    "    # and the number of descendants\n",
    "    numA = torch.zeros(nbins - 1)\n",
    "    # number of ancestors for each bin\n",
    "    numD = torch.zeros(nbins - 1)\n",
    "    # number of descendants for each ancestral bin\n",
    "    i = 0\n",
    "    while i < (numA.size(0) - 1):\n",
    "        numA[i] = torch.sum(spikes[:, i] == 1).item()\n",
    "        numD[i] = torch.sum(spikes[:, i + bin_size] == 1).item()\n",
    "\n",
    "        # Check if numA[i] is 0, and remove numA[i] and numD[i] if it is\n",
    "        if numA[i] == 0:\n",
    "            numA = torch.cat((numA[:i], numA[i+1:]))\n",
    "            numD = torch.cat((numD[:i], numD[i+1:]))\n",
    "        else:\n",
    "            i+=1\n",
    "\n",
    "    # the ratio of descendants per ancestor\n",
    "    d = numD / numA\n",
    "    bscore = torch.nanmean(d)\n",
    "    return bscore\n",
    "\n",
    "# Synchrony -- Fano Factor\n",
    "def fano_factor(seq_len, spike):\n",
    "    # Calculate value similar to the Fano factor to estimate synchrony quickly\n",
    "    # During each bin, calculate the variance of the number of spikes per neuron divided by the mean of the number of spikes per neuron\n",
    "    # The Fano factor during one interval is equal to the mean of the values calculated for each bin in it\n",
    "    # Spike should have dims of neuron, time\n",
    "    # Returned fano factor should have dims of trial\n",
    "    len_bins = 10  # ms\n",
    "    n_bins = int(round(seq_len / len_bins))\n",
    "    fano_all = torch.zeros(n_bins)\n",
    "    for i in range(n_bins):\n",
    "        spike_slice = spike[:, i * len_bins:(i + 1) * len_bins]\n",
    "        spikes_per_neuron = torch.sum(spike_slice, axis=1)\n",
    "        variance_spikes = torch.var(spikes_per_neuron)\n",
    "        mean_spikes = torch.mean(spikes_per_neuron)\n",
    "        fano_bin = variance_spikes / mean_spikes if mean_spikes != 0 else 0\n",
    "        fano_all[i] = fano_bin\n",
    "    n_fano = torch.mean(fano_all)\n",
    "    return n_fano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conn_mx(rows, columns, sparseness):\n",
    "    # Calculate the number of non-zero entries based on sparseness\n",
    "    num_non_zero_entries = int(rows * columns * sparseness)\n",
    "\n",
    "    # Initialize the matrix with zeros\n",
    "    conn_mx = torch.zeros(rows, columns)\n",
    "\n",
    "    # Randomly select indices to set to the specified value\n",
    "    indices = torch.randperm(rows * columns)[:num_non_zero_entries]\n",
    "\n",
    "    # Initialize non-zero values using log normal distribution\n",
    "    mu = -0.64\n",
    "    sigma = 0.51\n",
    "    log_normal_values = torch.empty(indices.shape).normal_(mean=mu, std=sigma).exp_()\n",
    "    conn_mx.view(-1)[indices] = log_normal_values\n",
    "\n",
    "    return conn_mx\n",
    "\n",
    "# creates an excitatory and inhibitory matrix\n",
    "def hid_mx(rows, columns, num_excitatory, num_inhibitory):\n",
    "    # hard coded sparsity\n",
    "\n",
    "    # Initialize the weight matrix\n",
    "    weight_matrix = np.zeros((num_excitatory + num_inhibitory, num_excitatory + num_inhibitory))\n",
    "\n",
    "    # Set excitatory to excitatory connections\n",
    "    weight_matrix[:num_excitatory, :num_excitatory] = np.random.choice([0, 1], size=(num_excitatory, num_excitatory), p=[1-0.16, 0.16])\n",
    "\n",
    "    # Set excitatory to inhibitory connections\n",
    "    weight_matrix[:num_excitatory, num_excitatory:] = np.random.choice([0, 1], size=(num_excitatory, num_inhibitory), p=[1-0.205, 0.205])\n",
    "\n",
    "    # Set inhibitory to excitatory connections\n",
    "    weight_matrix[num_excitatory:, :num_excitatory] = np.random.choice([0, -1], size=(num_inhibitory, num_excitatory),p=[1-0.252, 0.252])\n",
    "\n",
    "    # Set inhibitory to inhibitory connections\n",
    "    weight_matrix[num_excitatory:, num_excitatory:] = np.random.choice([0, -1], size=(num_inhibitory, num_inhibitory), p=[1-0.284, 0.284] )\n",
    "\n",
    "    # Initialize non-zero values using log normal distribution\n",
    "    mu = -0.64\n",
    "    sigma = 0.51\n",
    "    non_zero_indices = np.where(weight_matrix != 0)\n",
    "    weight_matrix[non_zero_indices] = np.random.lognormal(mean=mu, sigma=sigma, size=non_zero_indices[0].shape)\n",
    "\n",
    "    # Multiply the last num_inhibitory rows by -10\n",
    "    weight_matrix[-num_inhibitory:, :] *= -10\n",
    "\n",
    "    return torch.tensor(weight_matrix.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conn_mx(rows, columns, sparseness):\n",
    "    # Calculate the number of non-zero entries based on sparseness\n",
    "    num_non_zero_entries = int(rows * columns * sparseness)\n",
    "\n",
    "    # Initialize the matrix with zeros\n",
    "    conn_mx = torch.zeros(rows, columns)\n",
    "\n",
    "    # Randomly select indices to set to the specified value\n",
    "    indices = torch.randperm(rows * columns)[:num_non_zero_entries]\n",
    "\n",
    "    # Initialize non-zero values using log normal distribution\n",
    "    mu = -0.64\n",
    "    sigma = 0.51\n",
    "    log_normal_values = torch.empty(indices.shape).normal_(mean=mu, std=sigma).exp_()\n",
    "    conn_mx.view(-1)[indices] = log_normal_values\n",
    "\n",
    "    return conn_mx\n",
    "\n",
    "# creates an excitatory and inhibitory matrix\n",
    "def hid_mx(num_excitatory, num_inhibitory, num_iPV, num_iSst, num_iHtr, p_nn):\n",
    "\n",
    "    # Why are there so many neurons :( \n",
    "\n",
    "    # Initialize the weight matrix\n",
    "    weight_matrix = np.zeros((num_excitatory + num_inhibitory, num_excitatory + num_inhibitory))\n",
    "\n",
    "    # Excitatory connections\n",
    "\n",
    "    # excitatory to excitatory\n",
    "    weight_matrix[:num_excitatory, :num_excitatory] = np.random.choice([0, 1], size=(num_excitatory, num_excitatory), p=[1-p_nn['e_e'], p_nn['e_e']])\n",
    "    # excitatory to inhibitory PV\n",
    "    weight_matrix[:num_excitatory, num_excitatory:num_excitatory+num_iPV] = np.random.choice([0, 1], size=(num_excitatory, num_iPV), p=[1-p_nn['e_PV'], p_nn['e_PV']])\n",
    "    # excitatory to inhibitory Sst\n",
    "    weight_matrix[:num_excitatory, num_excitatory+num_iPV:num_excitatory+num_iPV+num_iSst] = np.random.choice([0, 1], size=(num_excitatory, num_iSst), p=[1-p_nn['e_Sst'], p_nn['e_Sst']])\n",
    "    # excitatory to inhibitory Htr\n",
    "    weight_matrix[:num_excitatory, num_excitatory+num_iPV+num_iSst:] = np.random.choice([0, 1], size=(num_excitatory, num_iHtr), p=[1-p_nn['e_Htr'], p_nn['e_Htr']])\n",
    "\n",
    "\n",
    "    # Inhibitory connections\n",
    "\n",
    "    # inhibitory PV to excitatory\n",
    "    weight_matrix[num_excitatory:num_excitatory+num_iPV, :num_excitatory] = np.random.choice([0, -1], size=(num_iPV, num_excitatory), p=[1-p_nn['PV_e'], p_nn['PV_e']])\n",
    "    # inhibitory PV to inhibitory PV\n",
    "    weight_matrix[num_excitatory:num_excitatory+num_iPV, num_excitatory:num_excitatory+num_iPV] = np.random.choice([0, -1], size=(num_iPV, num_iPV), p=[1-p_nn['PV_PV'], p_nn['PV_PV']])\n",
    "    # inhibitory PV to inhibitory Htr\n",
    "    weight_matrix[num_excitatory:num_excitatory+num_iPV, num_excitatory+num_iPV:num_excitatory+num_iPV+num_iSst] = np.random.choice([0, -1], size=(num_iPV, num_iSst), p=[1-p_nn['PV_Sst'], p_nn['PV_Sst']])\n",
    "    # inhibitory PV to inhibitory Sst\n",
    "    weight_matrix[num_excitatory:num_excitatory+num_iPV, num_excitatory+num_iPV+num_iSst:] = np.random.choice([0, -1], size=(num_iPV, num_iHtr), p=[1-p_nn['PV_Htr'], p_nn['PV_Htr']]) \n",
    "\n",
    "    # inhibitory Sst to excitatory\n",
    "    weight_matrix[num_excitatory+num_iPV:num_excitatory+num_iPV+num_iSst, :num_excitatory] = np.random.choice([0, -1], size=(num_iSst, num_excitatory), p=[1-p_nn['Sst_e'], p_nn['Sst_e']])\n",
    "    # inhibitory Sst to inhibitory PV\n",
    "    weight_matrix[num_excitatory+num_iPV:num_excitatory+num_iPV+num_iSst, num_excitatory:num_excitatory+num_iPV] = np.random.choice([0, -1], size=(num_iSst, num_iPV), p=[1-p_nn['Sst_PV'], p_nn['Sst_PV']])\n",
    "    # inhibitory Sst to inhibitory Htr\n",
    "    weight_matrix[num_excitatory+num_iPV:num_excitatory+num_iPV+num_iSst, num_excitatory+num_iPV:num_excitatory+num_iPV+num_iSst] = np.random.choice([0, -1], size=(num_iSst, num_iSst), p=[1-p_nn['Sst_Sst'], p_nn['Sst_Sst']])\n",
    "    # inhibitory Sst to inhibitory Sst\n",
    "    weight_matrix[num_excitatory+num_iPV:num_excitatory+num_iPV+num_iSst, num_excitatory+num_iPV+num_iSst:] = np.random.choice([0, -1], size=(num_iSst, num_iHtr), p=[1-p_nn['Sst_Htr'], p_nn['Sst_Htr']]) \n",
    "\n",
    "    # inhibitory Sst to excitatory\n",
    "    weight_matrix[num_excitatory+num_iPV+num_iSst:, :num_excitatory] = np.random.choice([0, -1], size=(num_iHtr, num_excitatory), p=[1-p_nn['Htr_e'], p_nn['Htr_e']])\n",
    "    # inhibitory Sst to inhibitory PV\n",
    "    weight_matrix[num_excitatory+num_iPV+num_iSst:, num_excitatory:num_excitatory+num_iPV] = np.random.choice([0, -1], size=(num_iHtr, num_iPV), p=[1-p_nn['Htr_PV'], p_nn['Htr_PV']])\n",
    "    # inhibitory Sst to inhibitory Htr\n",
    "    weight_matrix[num_excitatory+num_iPV+num_iSst:, num_excitatory+num_iPV:num_excitatory+num_iPV+num_iSst] = np.random.choice([0, -1], size=(num_iHtr, num_iSst), p=[1-p_nn['Htr_Sst'], p_nn['Htr_Sst']])\n",
    "    # inhibitory Sst to inhibitory Sst\n",
    "    weight_matrix[num_excitatory+num_iPV+num_iSst:, num_excitatory+num_iPV+num_iSst:] = np.random.choice([0, -1], size=(num_iHtr, num_iHtr), p=[1-p_nn['Htr_Htr'], p_nn['Htr_Htr']]) \n",
    "\n",
    "\n",
    "    # Initialize non-zero values using log normal distribution\n",
    "    mu = -0.64\n",
    "    sigma = 0.51\n",
    "    non_zero_indices = np.where(weight_matrix != 0)\n",
    "    weight_matrix[non_zero_indices] = np.random.lognormal(mean=mu, sigma=sigma, size=non_zero_indices[0].shape)\n",
    "\n",
    "    # Multiply the last num_inhibitory rows by -10\n",
    "    weight_matrix[-num_inhibitory:, :] *= -10\n",
    "\n",
    "    return torch.tensor(weight_matrix.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from snntorch import functional\n",
    "from snntorch import LIF\n",
    "\n",
    "class RLIF1(LIF):\n",
    "    def __init__(\n",
    "        self,\n",
    "        beta,\n",
    "        V=1.0,\n",
    "        all_to_all=True,\n",
    "        linear_features=None,\n",
    "        conv2d_channels=None,\n",
    "        kernel_size=None,\n",
    "        threshold=1,\n",
    "        spike_grad=None,\n",
    "        surrogate_disable=False,\n",
    "        init_hidden=False,\n",
    "        inhibition=False,\n",
    "        learn_beta=False,\n",
    "        learn_threshold=False,\n",
    "        learn_recurrent=True,  # changed learn_V\n",
    "        reset_mechanism=\"zero\",\n",
    "        state_quant=False,\n",
    "        output=False,\n",
    "        reset_delay=True,\n",
    "        refractory_period=5,  # in milliseconds\n",
    "    ):\n",
    "        super().__init__(\n",
    "            beta,\n",
    "            threshold,\n",
    "            spike_grad,\n",
    "            surrogate_disable,\n",
    "            init_hidden,\n",
    "            inhibition,\n",
    "            learn_beta,\n",
    "            learn_threshold,\n",
    "            reset_mechanism,\n",
    "            state_quant,\n",
    "            output,\n",
    "        )\n",
    "\n",
    "        self.all_to_all = all_to_all\n",
    "        self.learn_recurrent = learn_recurrent\n",
    "\n",
    "        # linear params\n",
    "        self.linear_features = linear_features\n",
    "\n",
    "        # Conv2d params\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv2d_channels = conv2d_channels\n",
    "\n",
    "        # catch cases\n",
    "        self._rleaky_init_cases()\n",
    "\n",
    "        # initialize recurrent connections\n",
    "        if self.all_to_all:  # init all-all connections\n",
    "            self._init_recurrent_net()\n",
    "        else:  # initialize 1-1 connections\n",
    "            self._V_register_buffer(V, learn_recurrent)\n",
    "            self._init_recurrent_one_to_one()\n",
    "\n",
    "        if not learn_recurrent:\n",
    "            self._disable_recurrent_grad()\n",
    "\n",
    "        self._init_mem()\n",
    "\n",
    "        if self.reset_mechanism_val == 0:  # reset by subtraction\n",
    "            self.state_function = self._base_sub\n",
    "        elif self.reset_mechanism_val == 1:  # reset to zero\n",
    "            self.state_function = self._base_zero\n",
    "        elif self.reset_mechanism_val == 2:  # no reset, pure integration\n",
    "            self.state_function = self._base_int\n",
    "\n",
    "        self.reset_delay = reset_delay\n",
    "\n",
    "        # Refractory period in timesteps\n",
    "        self.refractory_period = refractory_period\n",
    "\n",
    "    def _init_mem(self):\n",
    "        #initialize spike, membrane, and refractory counter\n",
    "        spk = torch.zeros(0)\n",
    "        mem = torch.zeros(0)\n",
    "        refractory_counter = torch.zeros(0)\n",
    "\n",
    "        self.register_buffer(\"spk\", spk, False)\n",
    "        self.register_buffer(\"mem\", mem, False)\n",
    "        #persistent=False, you are indicating that refractory_counter is an intermediate state that should not be included in the model's state_dic\n",
    "        self.register_buffer(\"refractory_counter\", refractory_counter, persistent=False)\n",
    "\n",
    "    def reset_mem(self):\n",
    "        self.spk = torch.zeros_like(self.spk, device=self.spk.device)\n",
    "        self.mem = torch.zeros_like(self.mem, device=self.mem.device)\n",
    "        self.refractory_counter = torch.zeros_like(self.refractory_counter, device=self.refractory_counter.device)\n",
    "        return self.spk, self.mem\n",
    "\n",
    "    def init_rleaky(self):\n",
    "        \"\"\"Deprecated, use :class:`RLeaky.reset_mem` instead\"\"\"\n",
    "        return self.reset_mem()\n",
    "\n",
    "    def forward(self, input_, spk=None, mem=None, refractory_counter=None):\n",
    "        if not spk is None:\n",
    "            self.spk = spk\n",
    "\n",
    "        if not mem is None:\n",
    "            self.mem = mem\n",
    "\n",
    "        if not refractory_counter is None:\n",
    "            self.refractory_counter = refractory_counter\n",
    "\n",
    "        if self.init_hidden and (not mem is None or not spk is None or not refractory_counter is None):\n",
    "            raise TypeError(\n",
    "                \"When `init_hidden=True`, RLeaky expects 1 input argument.\"\n",
    "            )\n",
    "\n",
    "        if not self.spk.shape == input_.shape:\n",
    "            self.spk = torch.zeros_like(input_, device=self.spk.device)\n",
    "\n",
    "        if not self.mem.shape == input_.shape:\n",
    "            self.mem = torch.zeros_like(input_, device=self.mem.device)\n",
    "\n",
    "        if not self.refractory_counter.shape == input_.shape:\n",
    "            self.refractory_counter = torch.zeros_like(input_, device=self.refractory_counter.device)\n",
    "\n",
    "        # With each forward, decrement the counter\n",
    "        self.refractory_counter = torch.clamp(self.refractory_counter - 1, min=0)\n",
    "\n",
    "        # Update the membrane potential\n",
    "        self.reset = self.mem_reset(self.mem)\n",
    "        self.mem = self.state_function(input_)\n",
    "\n",
    "        # set a spike on when refractory period is 0\n",
    "        refractory_mask = (self.refractory_counter == 0)\n",
    "        self.spk = self.fire(self.mem) * refractory_mask\n",
    "\n",
    "        # Update the refractory counter back to 5 where spikes occurred\n",
    "        self.refractory_counter[self.spk > 0] = self.refractory_period\n",
    "\n",
    "        if not self.reset_delay:\n",
    "            do_reset = (\n",
    "                self.spk / self.graded_spikes_factor - self.reset\n",
    "            )  # avoid double reset\n",
    "            if self.reset_mechanism_val == 0:  # reset by subtraction\n",
    "                self.mem = self.mem - do_reset * self.threshold\n",
    "            elif self.reset_mechanism_val == 1:  # reset to zero\n",
    "                self.mem = self.mem - do_reset * self.mem\n",
    "\n",
    "        if self.output:\n",
    "            return self.spk, self.mem\n",
    "        elif self.init_hidden:\n",
    "            return self.spk\n",
    "        else:\n",
    "            return self.spk, self.mem\n",
    "\n",
    "    def _init_recurrent_net(self):\n",
    "        if self.all_to_all:\n",
    "            if self.linear_features:\n",
    "                self._init_recurrent_linear()\n",
    "            elif self.kernel_size is not None:\n",
    "                self._init_recurrent_conv2d()\n",
    "        else:\n",
    "            self._init_recurrent_one_to_one()\n",
    "\n",
    "    def _init_recurrent_linear(self):\n",
    "        self.recurrent = nn.Linear(self.linear_features, self.linear_features)\n",
    "\n",
    "    def _init_recurrent_conv2d(self):\n",
    "        self._init_padding()\n",
    "        self.recurrent = nn.Conv2d(\n",
    "            in_channels=self.conv2d_channels,\n",
    "            out_channels=self.conv2d_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "\n",
    "    def _init_padding(self):\n",
    "        if type(self.kernel_size) is int:\n",
    "            self.padding = self.kernel_size // 2, self.kernel_size // 2\n",
    "        else:\n",
    "            self.padding = self.kernel_size[0] // 2, self.kernel_size[1] // 2\n",
    "\n",
    "    def _init_recurrent_one_to_one(self):\n",
    "        self.recurrent = RecurrentOneToOne(self.V)\n",
    "\n",
    "    def _disable_recurrent_grad(self):\n",
    "        for param in self.recurrent.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _base_state_function(self, input_):\n",
    "        base_fn = (\n",
    "            self.beta.clamp(0, 1) * self.mem\n",
    "            + input_\n",
    "            + self.recurrent(self.spk)\n",
    "        )\n",
    "        return base_fn\n",
    "\n",
    "    def _base_sub(self, input_):\n",
    "        return self._base_state_function(input_) - self.reset * self.threshold\n",
    "\n",
    "    def _base_zero(self, input_):\n",
    "        return self._base_state_function(input_) - self.reset * self._base_state_function(input_)\n",
    "\n",
    "    def _base_int(self, input_):\n",
    "        return self._base_state_function(input_)\n",
    "\n",
    "    def _rleaky_init_cases(self):\n",
    "        all_to_all_bool = bool(self.all_to_all)\n",
    "        linear_features_bool = self.linear_features\n",
    "        conv2d_channels_bool = bool(self.conv2d_channels)\n",
    "        kernel_size_bool = bool(self.kernel_size)\n",
    "\n",
    "        if all_to_all_bool:\n",
    "            if not (linear_features_bool):\n",
    "                if not (conv2d_channels_bool or kernel_size_bool):\n",
    "                    raise TypeError(\n",
    "                        \"When `all_to_all=True`, RLeaky requires either\"\n",
    "                        \"`linear_features` or (`conv2d_channels` and \"\n",
    "                        \"`kernel_size`) to be specified. The \"\n",
    "                        \"shape should match the shape of the output spike of \"\n",
    "                        \"the layer.\"\n",
    "                    )\n",
    "                elif conv2d_channels_bool ^ kernel_size_bool:\n",
    "                    raise TypeError(\n",
    "                        \"`conv2d_channels` and `kernel_size` must both be\"\n",
    "                        \"specified. The shape of `conv2d_channels` should \"\n",
    "                        \"match the shape of the output\"\n",
    "                        \"spikes.\"\n",
    "                    )\n",
    "            elif (linear_features_bool and kernel_size_bool) or (\n",
    "                linear_features_bool and conv2d_channels_bool\n",
    "            ):\n",
    "                raise TypeError(\n",
    "                    \"`linear_features` cannot be specified at the same time as\"\n",
    "                    \"`conv2d_channels` or `kernel_size`. A linear layer and \"\n",
    "                    \"conv2d layer cannot both\"\n",
    "                    \"be specified at the same time.\"\n",
    "                )\n",
    "        else:\n",
    "            if (\n",
    "                linear_features_bool\n",
    "                or conv2d_channels_bool\n",
    "                or kernel_size_bool\n",
    "            ):\n",
    "                raise TypeError(\n",
    "                    \"When `all_to_all`=False, none of `linear_features`,\"\n",
    "                    \"`conv2d_channels`, or `kernel_size` should be specified. \"\n",
    "                    \"The weight `V` is used\"\n",
    "                    \"instead.\"\n",
    "                )\n",
    "\n",
    "    @classmethod\n",
    "    def detach_hidden(cls):\n",
    "        \"\"\"Returns the hidden states, detached from the current graph.\n",
    "        Intended\n",
    "        for use in truncated backpropagation through time where hidden state\n",
    "        variables\n",
    "        are instance variables.\"\"\"\n",
    "\n",
    "        for layer in range(len(cls.instances)):\n",
    "            if isinstance(cls.instances[layer], RLIF1):\n",
    "                cls.instances[layer].mem.detach_()\n",
    "                cls.instances[layer].spk.detach_()\n",
    "\n",
    "    @classmethod\n",
    "    def reset_hidden(cls):\n",
    "        \"\"\"Used to clear hidden state variables to zero.\n",
    "        Intended for use where hidden state variables are instance variables.\n",
    "        Assumes hidden states have a batch dimension already.\"\"\"\n",
    "        for layer in range(len(cls.instances)):\n",
    "            if isinstance(cls.instances[layer], RLIF1):\n",
    "                (\n",
    "                    cls.instances[layer].spk,\n",
    "                    cls.instances[layer].mem,\n",
    "                ) = cls.instances[layer].init_rleaky()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CustomLoss_task(nn.Module):\n",
    "\n",
    "    def __init__(self, target_firing_rate=0.02,target_synchrony=1.4, target_branching=1.0,batch_size=25):\n",
    "        super(CustomLoss_task, self).__init__()\n",
    "        self.target_synchrony = torch.tensor([target_synchrony] * batch_size, requires_grad=True)\n",
    "        self.target_firing_rate = torch.tensor([target_firing_rate] * batch_size,requires_grad=True)\n",
    "        self.target_branching = torch.tensor([target_branching] * batch_size,requires_grad=True)\n",
    "        self.criterion_classification = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets, firing_rate, synchrony_fano_factor, criticality):\n",
    "\n",
    "        w_task = 1\n",
    "        task_loss = self.criterion_classification(outputs, targets)\n",
    "        self.task_loss = task_loss\n",
    "\n",
    "        self.rate_loss = torch.tensor([0])\n",
    "        self.criticality_loss = torch.tensor([0])\n",
    "        self.synchrony_loss = torch.tensor([0])\n",
    "\n",
    "        total_loss = w_task*task_loss \n",
    "        return total_loss\n",
    "\n",
    "class CustomLoss_task_rate(nn.Module):\n",
    "\n",
    "    def __init__(self, target_firing_rate=0.02, target_synchrony=1.4, target_branching=1.0,batch_size=25):\n",
    "        super(CustomLoss_task_rate, self).__init__()\n",
    "        self.target_synchrony = torch.tensor([target_synchrony] * batch_size, requires_grad=True)\n",
    "        self.target_firing_rate = torch.tensor([target_firing_rate] * batch_size,requires_grad=True)\n",
    "        self.target_branching = torch.tensor([target_branching] * batch_size,requires_grad=True)\n",
    "        self.criterion_classification = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets, criticality, firing_rate, synchrony_fano_factor):\n",
    "        w_rate = 0\n",
    "        w_task = 1\n",
    "        \n",
    "        task_loss = self.criterion_classification(outputs, targets)\n",
    "        rate_loss = nn.MSELoss()(firing_rate,self.target_firing_rate)\n",
    "\n",
    "        self.task_loss = task_loss\n",
    "        self.rate_loss = rate_loss\n",
    "\n",
    "        self.criticality_loss = torch.tensor([0])\n",
    "        self.synchrony_loss = torch.tensor([0])\n",
    "\n",
    "        total_loss = w_task*task_loss + w_rate*rate_loss\n",
    "        return total_loss\n",
    "    \n",
    "class CustomLoss_task_sync(nn.Module):\n",
    "\n",
    "    def __init__(self,target_firing_rate=0.02,  target_synchrony=1.4, target_branching=1.0,batch_size=25):\n",
    "        super(CustomLoss_task_sync, self).__init__()\n",
    "        self.target_synchrony = torch.tensor([target_synchrony] * batch_size, requires_grad=True)\n",
    "        self.target_firing_rate = torch.tensor([target_firing_rate] * batch_size,requires_grad=True)\n",
    "        self.target_branching = torch.tensor([target_branching] * batch_size,requires_grad=True)\n",
    "        self.criterion_classification = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets, criticality, firing_rate, synchrony_fano_factor):\n",
    "        w_sync = 0\n",
    "        w_task = 1\n",
    "        \n",
    "        task_loss = self.criterion_classification(outputs, targets)\n",
    "        synchrony_loss = nn.MSELoss()(synchrony_fano_factor,self.target_synchrony)\n",
    "\n",
    "        self.task_loss = task_loss\n",
    "        self.synchrony_loss = synchrony_loss\n",
    "\n",
    "        self.rate_loss = torch.tensor([0])\n",
    "        self.criticality_loss = torch.tensor([0])\n",
    "\n",
    "        total_loss = w_task*task_loss + w_sync*synchrony_loss\n",
    "        return total_loss\n",
    "\n",
    "class CustomLoss_task_criticality(nn.Module):\n",
    "\n",
    "    def __init__(self, target_firing_rate=0.02, target_synchrony=1.4,target_branching=1.0,batch_size=25):\n",
    "        super(CustomLoss_task_criticality, self).__init__()\n",
    "        self.target_synchrony = torch.tensor([target_synchrony] * batch_size, requires_grad=True)\n",
    "        self.target_firing_rate = torch.tensor([target_firing_rate] * batch_size,requires_grad=True)\n",
    "        self.target_criticality = torch.tensor([target_branching] * batch_size,requires_grad=True)\n",
    "        self.criterion_classification = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets, criticality, firing_rate, synchrony_fano_factor):\n",
    "        w_crit = 0\n",
    "        w_task = 1\n",
    "        \n",
    "        task_loss = self.criterion_classification(outputs, targets)\n",
    "        criticality_loss = nn.MSELoss()(criticality,self.target_criticality)\n",
    "\n",
    "        self.task_loss = task_loss\n",
    "        self.criticality_loss = criticality_loss\n",
    "\n",
    "        self.rate_loss = torch.tensor([0])\n",
    "        self.synchrony_loss = torch.tensor([0])\n",
    "\n",
    "        total_loss = w_task*task_loss + w_crit*criticality_loss\n",
    "        return total_loss\n",
    "    \n",
    "class CustomLoss_all(nn.Module):\n",
    "\n",
    "    def __init__(self, target_firing_rate=0.02, target_synchrony=1.4, target_branching=1.0,batch_size=25):\n",
    "        super(CustomLoss_all, self).__init__()\n",
    "        self.target_synchrony = torch.tensor([target_synchrony] * batch_size, requires_grad=True)\n",
    "        self.target_firing_rate = torch.tensor([target_firing_rate] * batch_size,requires_grad=True)\n",
    "        self.target_branching = torch.tensor([target_branching] * batch_size,requires_grad=True)\n",
    "        self.criterion_classification = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets, criticality, firing_rate, synchrony_fano_factor):\n",
    "\n",
    "        w_crit = 0\n",
    "        w_rate = 0\n",
    "        w_sync = 0\n",
    "        w_task = 1\n",
    "        \n",
    "        task_loss = self.criterion_classification(outputs, targets)\n",
    "        rate_loss = nn.MSELoss()(firing_rate, self.target_firing_rate)\n",
    "        criticality_loss = nn.MSELoss()(criticality,self.target_branching)\n",
    "        synchrony_loss = nn.MSELoss()(synchrony_fano_factor,self.target_synchrony)\n",
    "\n",
    "        self.task_loss = task_loss\n",
    "        self.rate_loss = rate_loss\n",
    "        self.criticality_loss = criticality_loss\n",
    "        self.synchrony_loss = synchrony_loss\n",
    "\n",
    "        total_loss = w_task*task_loss + w_rate*rate_loss + w_crit*criticality_loss + w_sync*synchrony_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RSNN3, self).__init__()\n",
    "        num_inputs = 700\n",
    "        num_hidden = 1000\n",
    "        num_output = 20\n",
    "        beta = 0.85\n",
    "        pe_e = 0.16\n",
    "\n",
    "        # Define the dimensions\n",
    "        num_excitatory = 800\n",
    "        self.num_excitatory = num_excitatory\n",
    "        num_inhibitory = 200\n",
    "        self.false_neg = []\n",
    "        self.false_pos = []\n",
    "\n",
    "        #input to hidden layer\n",
    "        input_hid_mx = conn_mx(num_inputs, num_hidden, pe_e)\n",
    "        self.input_hid_mx = input_hid_mx\n",
    "        self.l1 = nn.Linear(num_inputs,num_hidden)\n",
    "        init.kaiming_uniform_(self.l1.weight)\n",
    "        self.l1.weight.data = input_hid_mx.T\n",
    "\n",
    "        # Recurrent layer weight matrix\n",
    "        hidden_mx = hid_mx(num_hidden,num_hidden,num_excitatory,num_inhibitory)\n",
    "        self.rlif1 = RLIF1(reset_mechanism=\"zero\",threshold=1, beta=beta, linear_features=num_hidden, all_to_all=True)\n",
    "        self.rlif1.recurrent.weight.data = hidden_mx.T\n",
    "\n",
    "        #hidden to output layer\n",
    "        hid_out_mx = conn_mx(num_hidden,num_output,pe_e)\n",
    "        self.l2 = nn.Linear(num_hidden, num_output)\n",
    "        init.kaiming_uniform_(self.l2.weight)\n",
    "        self.l2.weight.data = hid_out_mx.T\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        spk1,mem1 = self.rlif1.init_rleaky()\n",
    "        self.spk1_rec = []\n",
    "        self.cur2_rec = []\n",
    "\n",
    "        # print(inputs.shape)\n",
    "        for step in range(inputs.shape[0]): #300\n",
    "            cur_input = inputs[step,:]\n",
    "            cur1 = self.l1(cur_input)\n",
    "            spk1, mem1 = self.rlif1(cur1, spk1, mem1)\n",
    "            cur2 = self.l2(spk1)\n",
    "\n",
    "            self.spk1_rec.append(spk1)\n",
    "            self.cur2_rec.append(cur2)\n",
    "\n",
    "        self.spk1_rec = torch.stack(self.spk1_rec)\n",
    "        self.cur2_rec = torch.stack(self.cur2_rec)\n",
    "        cur2_rec = self.cur2_rec.mean(dim = 0)\n",
    "\n",
    "        output = nn.functional.softmax(cur2_rec,dim=-1)\n",
    "\n",
    "        return output, self.spk1_rec\n",
    "\n",
    "    def positive_negative_weights(self):\n",
    "\n",
    "        excitatory_weights = self.rlif1.recurrent.weight.data[:, :self.num_excitatory]\n",
    "        inhibitory_weights = self.rlif1.recurrent.weight.data[:, self.num_excitatory:]\n",
    "\n",
    "        #save the number of positives in inhibitory and negatives in excitatory region\n",
    "        num_false_neg = torch.sum(excitatory_weights < 0).item()\n",
    "        num_false_pos = torch.sum(inhibitory_weights > 0).item()\n",
    "\n",
    "        self.false_neg.append(num_false_neg)\n",
    "        self.false_pos.append(num_false_pos)\n",
    "\n",
    "        # Clamp switched sign values at 0\n",
    "        excitatory_weights.clamp_(min=0)\n",
    "        inhibitory_weights.clamp_(max=0)\n",
    "\n",
    "        mu = -0.64\n",
    "        sigma = 0.51\n",
    "\n",
    "        excitatory_zero_indices = (self.rlif1.recurrent.weight.data[:, :self.num_excitatory] == 0).nonzero(as_tuple=True)\n",
    "        inhibitory_zero_indices = (self.rlif1.recurrent.weight.data[:, self.num_excitatory:] == 0).nonzero(as_tuple=True)\n",
    "\n",
    "        if (len(excitatory_zero_indices) > num_false_pos):\n",
    "            excitatory_sampled_indices = torch.stack([\n",
    "                    excitatory_zero_indices[0][torch.randint(len(excitatory_zero_indices[0]), (num_false_pos,))],\n",
    "                    excitatory_zero_indices[1][torch.randint(len(excitatory_zero_indices[1]), (num_false_pos,))]\n",
    "                ], dim=1)\n",
    "\n",
    "            # generating self.excitatory_changes number of lognormal values\n",
    "            new_excitatory_values = torch.from_numpy(np.random.lognormal(mean=mu, sigma=sigma, size=num_false_pos)).float()\n",
    "            self.rlif1.recurrent.weight.data[excitatory_sampled_indices[:, 0], excitatory_sampled_indices[:, 1]] = new_excitatory_values\n",
    "\n",
    "        if (len(inhibitory_zero_indices) > num_false_neg):\n",
    "            inhibitory_sampled_indices = torch.stack([\n",
    "                    inhibitory_zero_indices[0][torch.randint(len(inhibitory_zero_indices[0]), (num_false_neg,))],\n",
    "                    inhibitory_zero_indices[1][torch.randint(len(inhibitory_zero_indices[1]), (num_false_neg,))]\n",
    "                ], dim=1)\n",
    "\n",
    "            new_inhibitory_values = -torch.from_numpy(np.random.lognormal(mean=mu, sigma=sigma, size=num_false_neg)).float()\n",
    "            self.rlif1.recurrent.weight.data[inhibitory_sampled_indices[:, 0], self.num_excitatory + inhibitory_sampled_indices[:, 1]] = new_inhibitory_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RSNN3(\n",
       "  (l1): Linear(in_features=700, out_features=1000, bias=True)\n",
       "  (rlif1): RLIF1(\n",
       "    (recurrent): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  )\n",
       "  (l2): Linear(in_features=1000, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RSNN3()\n",
    "criterion = CustomLoss_all()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.005)\n",
    "num_timesteps = 250\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_grad = []\n",
    "rec_grad = []\n",
    "output_grad = []\n",
    "save_spikes =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, num_epochs):\n",
    "    epoch_losses =[]\n",
    "    acc = []\n",
    "    for epoch in range(1, num_epochs):\n",
    "        predicted_outputs =[]\n",
    "        total_spikes = []\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            print(f\"Inputs shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.requires_grad_(True)\n",
    "            outputs = []\n",
    "            predictions =[]\n",
    "            # firing_rate_per_batch =[]\n",
    "            # criticality_per_batch =[]\n",
    "            # synchrony_per_batch =[]\n",
    "            spikes_per_batch = []\n",
    "            \n",
    "            for i in range(64):\n",
    "                output, spikes = model(inputs[i])\n",
    "                # print(f\"Output shape: {output.shape}, Spikes shape: {spikes.shape}\")\n",
    "                outputs.append(output)\n",
    "                spikes_per_batch.append(spikes)\n",
    "                spikes = spikes.squeeze()\n",
    "                if output.dim() == 1:\n",
    "                    output = output.unsqueeze(0)\n",
    "                    \n",
    "                _, predicted = torch.max(output.data, 1) # Get the predicted class (maximum value) from the output\n",
    "                predictions.append(predicted)\n",
    "\n",
    "            predicted_outputs = torch.cat(predictions).squeeze()\n",
    "            correct += (predicted_outputs.eq(labels)).sum().item()  # Compare predicted class with the true label and update the correct count\n",
    "            \n",
    "        \n",
    "            outputs = torch.stack(outputs)\n",
    "            labels = labels.long()\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()  \n",
    "            total += labels.size(0)\n",
    "            accuracy = 100. * correct / total\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "            acc.append(accuracy)\n",
    "            total_spikes= torch.stack(spikes_per_batch)\n",
    "        \n",
    "        if epoch in [1,10,20,30,40,50,60,70,80,90,100]:\n",
    "            save_spikes.append(total_spikes)\n",
    "            input_grad.append(model.l1.weight.grad.clone().cpu().numpy())\n",
    "            rec_grad.append(model.rlif1.recurrent.weight.grad.clone().cpu().numpy())\n",
    "            output_grad.append(model.l2.weight.grad.clone().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(1, num_epochs +1), epoch_losses, marker = 'o')\n",
    "    plt.title('Loss Change Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([64, 250, 700]), Labels shape: torch.Size([64])\n",
      "Epoch [1/100], Batch [0], Loss: 2.9984, Accuracy: 3.12%\n",
      "Inputs shape: torch.Size([64, 250, 700]), Labels shape: torch.Size([64])\n",
      "Epoch [1/100], Batch [1], Loss: 5.9942, Accuracy: 3.91%\n",
      "Inputs shape: torch.Size([64, 250, 700]), Labels shape: torch.Size([64])\n",
      "Epoch [1/100], Batch [2], Loss: 8.9901, Accuracy: 4.17%\n",
      "Inputs shape: torch.Size([64, 250, 700]), Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "acc = train(model, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsnn_env",
   "language": "python",
   "name": "rsnn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
